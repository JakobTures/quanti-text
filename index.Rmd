--- 
title: "Quantitative Textanalyse mit R"
author: "Jakob Tures"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
link-citations: yes
github-repo: https://github.com/JakobTures/quanti-text.git
---

``` {r, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(cache.path = 'cache/')
```

# Einleitung {-}

## Quantitative Textanalyse {-}

Quantitative Textanalyse umfasst die Anwendung statistischer Methoden auf -- in
erster Linie -- geschriebene Sprache. Darunter fallen unter anderem das einfache
Zählen von Worthäufigkeiten, die Analyse von Korrelationen und Netzwerken
zwischen Wörtern, das Erkennen der emotionalen Haltung in geschriebener Sprache
und die automatisierte Einordnung von Texten in latente oder explizite
Kategorien. Diese und andere Techniken der quantiativen Textanalyse werden wir
in diesem Kurs behandeln.

Texte als Datenbasis empirischer Wissenschaft kommen vor allem in der
qualitativen Forschung zum Einsatz. Hier werden Texte "per Hand" nach Ihren
Bedutungsinhalten kodiert oder kategorisiert. Dies hat den Vorteil, dass Sprache
in ihrem Kontext und ihrer enormen Komplexität von Menschen verstanden wird und
Ebenen analysiert werden können die tiefer liegen als das reine geschriebene
Wort es ausdrücken könnte. In der quantitativen Textanalyse nutzen wir hingegen
vor allem computergestützte Methoden. Computer können menschliche Sprache jedoch
nicht in dem Sinne *verstehen*, wie eine menschliche Kodierer:in dies kann.
Möchten wir beispielsweise den emotionalen Gehalt eines Textes -- das
**sentiment** -- analysieren, können wir ein dictionary mit nach Emotionen
kodierten Wörtern an unsere Textdaten anlegen, die Relation von "positiven" und
"negativen" Wörtern berechnen und Aussagen zu dem durchschnittlichen sentiment
eines Textes treffen. Da bei diesen relativ einfachen Techniken in der Regel
nur einzelne Wörter für sich betrachtet werden, kann der Computer bereits
Probleme mit der Verneinung von Begriffen haben. "Nicht gut" können wir als
Menschen sofort als negativ verstehen während unsere Software "nicht" und "gut"
getrennt betrachtet und so im besten Fall ein neutrales, im schlimmsten Fall ein
positives sentiment erkennt. Noch schwieriger wird es Ironie, Humor,
Anspielungen usw. adäquat mit den Mitteln der quantitiven Analyse zu erfassen,
eine Aufgabe die vor allem in der geschriebenen Sprache bereits für Menschen
schwierig sein kann -- vermutlich kennt jeder von uns die Situation, das
sentiment von Textnachrichten falsch ausgelegt zu haben. Auch bei der Anwendung
der ausgfeilteren Methoden quantitativer Textanalyse müssen wir uns dieser
Beschränkungen stets bewusst sein. Unsere Techniken und Algorithmen werden die
Fähigkeit menschlicher Kodierer:innen Sprache in ihrem Kontext zu verstehen
nicht erreichen.

Somit drängt sich natürlich die Frage zwingend auf, warum sollten wir überhaupt
Sprache computergestützt analysieren? Auch wenn wir die beschriebenen
Einschränkungen nicht "wegrationalisieren" können, bringt die quantiative
Textanalyse auch enorme Vorteile mit sich. Zunächst steht uns ein umfangreicher
Werkzeugkoffer voller hochentwickelter Methoden zur Verfügung um zum Einen den
Fehler gering zu halten und diesen zum Anderen quantifizierbar zu machen. Auch
überlassen wir den Computer in der Textanalyse nicht sich selbst, sondern
treffen bewusste Entscheidungen in der Gestaltung der Analyse und interpretieren
die Ergebnisse im Bewusstsein der Komplexität von Sprache und der Grenzen
unserer Methoden mit kritischem Blick.

Wir können also mit den Einschränkungen der Methode bewusst und möglichst
objektiv umgehen, dies ist aber weiterhin noch kein Argument *für* die
quantitative Textanalyse. Was diese aber im Gegensatz zu qualitativer
Textanalyse möglich macht, ist die Analyse großer Datenmengen in kurzer Zeit.
Menschliches Kodieren von Texten ist sehr zeitaufwendig und beschränkt sich in
der Regel auf eine geringe Anzahl von Dokumenten. In der computergestützten
Textanalyse können wir hingegen enorm große Textmengen in relativ wenig Zeit
analysieren. Wenn ein sehr großes Analyseprojekt mit aufwendigen Algorithmen
mehrere Stunden Rechenzeit in Anspruch nimmt -- diese Größenordnung werden wir
in disem Kurs nicht erreichen -- steht dies im Verhältnis zu mehreren Jahren
Arbeitszeit menschlicher Kodierer:innen. Im Rahmen dieses Kurses werden wir die
Plenarprotokolle des 19. Deutschen Bundestags analysieren. Dies sind mehr als
200 Protokolle mit jeweils bis zu mehreren hundert Seiten Text. Diese per Hand
zu analysieren ist mehr oder weniger unmöglich. Computergestützt wird aus
"unmöglich" jedoch nicht nur "machbar", sondern wir können uns während der
Rechenzeit sogar noch einen Kaffee holen. 

Ein weiteres starkes Argument für quantitative Textanalyse ist aus meiner Sicht
die Nachvollziehbarkeit und Transparenz der Analyse. Jede Nutzer:in mit Zugang
zu den Daten und unserem R-scripts kann die Analyse replizieren oder auch
modifizieren. Diese Replizierbarkeit ist eine fundamentale Säule empirischer
Wissenschaft und unabhängig von der nicht in Frage gestellten Fähigkeit und
Gründlichkeit qualitativer Forscher:innen dort in dieser Form nicht möglich.

Eine letzte einleitende Bemerkung: Auch wenn ich in den obrigen zeilen viele
Vergleiche zur qualitativen Textanalyse -- der klassischen "Heimat" von
Textanalysen -- gezogen habe, möchte ich klar betonen, dass es mir weder um eine
Aufwertung des einen noch um eine Abwertung des anderen Ansatzes geht.
Qualitative Methoden eröffnen Fragestellungen die den quantitativen verschlossen
bleiben und andersherum. Die relevante Frage ist also nicht, welcher Ansatz ist
"besser", sondern welcher ist adäquat für mein Forschungsinteresse? 


### Beispiele {-}


## Kursinhalte {-}

Der erste Block des Kurses (Kapitel \@ref(R1), \@ref(R2), \@ref(R3) & \@ref(R4))
umfasst eine Einführung in R und RStudio. R ist eine Programmiersprache die vor
allem für statistische Analysen Anwendung findet. Die Grundfunktionen von R
können durch das Einbinden von professionellen und nutzergeschriebenen
*packages* erweitert werden. Diese Erweiterungen sind es, die R so flexibel
machen und neben statistischen Analysen unter anderem auch das Schreiben von
Websites direkt in R -- so wie diese Website -- und die Aufbereitung und Analyse
von Textdaten ermöglichen. RStudio ist ein IDE -- 
*integrated development environment* -- für R und vereinfacht das Arbeiten mit
der Sprache. Der erste Block umfasst Informationen zur Installation beider
Softwares, der Bedienung von RStudio sowie grundlegenden R Befehlen.
Abgeschlossen wird er mit einem Überblick zur Transformation von Daten und deren
graphischer Analyse mit dem verbreiteten R package **tidyverse**.

Kapitel \@ref(BT19) befasst sich mit dem Datensatz der im Kurs zur Textanalyse
verwendet wird, den Plenarprotokollen des 19. Deutschen Bundestags. Wir
betrachten den Inhalt des Datensatzes sowie dessen Struktur.

Im dritten Block (Kapitel \@ref(tidytext1), \@ref(tidytext2), \@ref(tidytext3) &
\@ref(tidytext4)) beginnt die eigentliche Textanalyse. Wir nutzen dazu das
**tidytext** package, welches nach der Logik des **tidyverse** funktioniert und
sich somit sehr gut in dessen datenanalytischen workflow integriert. Wir
beginnen mit dem Umgang mit Textdaten und der statistischen Analyse einzelner
Wörter bevor wir uns der Arbeit mit Wortkombinationen widmen. Abgeschlossen wird
der Block mit Techniken zur sentiment Analyse anhand eines dictionaries.

Block Vier (Kapitel \@ref(quanteda1), \@ref(quanteda2) & \@ref(quanteda3)) führt
ein weiteres package zur quantitativen Textanalyse ein, **quanteda**. Dieses
nutzt eine alternative Struktur für Textdaten, deren Prinzipien wir beleuchten
und uns damit auseinandersetzen, wie wir sie mit dme **tidyverse** integrieren
können. Im Weiteren befassen wir uns mit grundlegenden Textanalyse Techniken in
**quanteda** und vor allem mit Methoden die über die Möglichkeiten von
**tidytext** hinausgehen.

Der abschließenden fünften Block (Kapitel \@ref(ML1) & \@ref(ML2)) führt in die
Techniken des *machine learnings* ein. Wir betrachten wie wir *unsupervised*
machine learning nutzen können um latente Dimensionen in unkategorisierten
Textdaten zu erkennen und diese automatisch nach Gemeinsamkeiten und
Unterschiedenen zu gruppieren. Im *supervised* machine learning geben wir für
einen Teil der Daten Kategorien vor um mit dem so trainierten Modell neue
ungesehene Daten zu kategorieseren. Dabei gehen wir auch auf die Unterschiede
und potentiellen anwendungsfälle beider Ansätze ein.



## Konventionen {-}

Die R Welt ist eine englischssprachige Welt. Die Namen von packages und
Funktionen sind meist mehr oder weniger sprechend und stets Englisch. Auch sind
die meisten Begriffe die sich auf R und RStudio beziehen englischsprachig.
Sinnvolle Übersetzungen existieren meist nicht. Auch wäre es nicht zielführend
diese Begriffe selbst zu übersetzen da dies nur unnötig verwirren würde. Einer
der wichtigsten Skills die Sie im Umgang mit einer Sprache wie R entwickeln
müssen, ist das gezielte googlen nach Problemen bzw. Lösungen. Dazu benötigen
Sie das englischsprachige Fachvokabular. Dies alles spräche dafür diese
Einführung in Englisch zu verfassen. Da aber das Datennmaterial, die
Plenarprotokolle, Deutsche Texte sind, blieb als Option nur der Kompromiss die
Seite auf Deutsch zu schreiben und die englischsprachigen Fachbegriffe
einzubinden ohne sie zu übersetzen.

Die Namen von R packages werden fett geschrieben. Da R *case sensitive*,
also Groß- und Kleinschreibung nicht beliebig austauschbar ist, werden die Namen
der packages exakt so geschrieben, wie sie benannt sind. Ein Beispiel ist:
**tidytext**

Alle Codebeispiele sind in *code font* gesetzt. Teilweise im Text:
`print("Hello World")`, teilweise als *code block*:

``` {r hello_world_1}
print("Hello World!")
```

Der output des R codes wird direkt innerhalb des code blocks hinter `##`
gedruckt.

Ich empfehle dringend den Inhalt der code blocks selbst in RStudio laufen zu
lassen und den code dabei auch selbst zu tippen statt ihn zu kopieren. Um eine
Sprache wir R zu erlernen, muss man sie regelmäßig selbst tippen. Nur so können
die Namen häufig genutzter Funktionen sowie die R syntax in Ihr "muscle memory"
übergehen.


## Danksagung {-}

Besonderer Dank gilt Lukas Höttges für die Unterstützung bei der Erstellung
dieser Website und der Durchführung des korrespondierenden Seminars an der
Universität Potsdam im Wintersemester 2021/22. Gleichermaßen danke ich den
aktuellen und ehemaligen Teammitgliedern des Lehrstuhls für Methoden der
empirischen Sozialforschung an der Universität Potsdam für ihr wertvolles
Feedback zu technischen und inhaltlichen Fragen in diversen Kaffeepausen und
Zoom-Gesprächen.

Dank gilt auch den Erstellern der diversen genutzten R packages, insbesondere
**bookdown**, **tidyverse**, **tidytext** und **quanteda** sowie der gesamten
R community.

## Colophon {-}

``` {r package_load_colophon, include = FALSE}
library(knitr)
library(tidyverse)
library(tidytext)
library(quanteda)
library(bookdown)
```

Bei der Gestaltung dieser website sowie der vorgestellten code Beispiele kam
Folgendes zum Einsatz:

``` {r colophon}
sessioninfo::session_info()
```
